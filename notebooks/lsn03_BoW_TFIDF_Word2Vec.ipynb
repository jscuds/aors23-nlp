{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Fundamentals (Day 3)\n",
    "\n",
    "**Goal:** Participants leave with \"lexical\" NLP skills to go from raw text data to insights to enable effective decision making.\n",
    "\n",
    "_Lexical Analysis: the meaning of a word in isolation from the sentence containing it._ \n",
    "\n",
    "_Semantic Analysis: determining what a sentence or phrase really means based on contextual clues._  \n",
    "\n",
    "**Day 1\\-2 Objectives:**\n",
    "\n",
    "1. ~~Understand common analytical tasks associated with NLP~~\n",
    "2. ~~Understand how text extraction can be completed~~\n",
    "3. ~~Understand and conduct common pre\\-processing NLP tasks \\(tokenization, spelling correction, stop word removal, stemming, lemmatization, casing, removal of digits / punctuation, etc.~~\n",
    "4. ~~Understand and complete common lexical NLP analytical tasks~~\n",
    "\n",
    "- <span style='font-size:small'>~~Part of Speech Tagging~~</span>\n",
    "- <span style='font-size:small'>~~N\\-gram formation~~</span>\n",
    "- <span style='font-size:small'>~~Sentiment Analysis \\(Bag of Words, lexicons \\(Vader, Bing, NRC, AFINN, Loughran\\)\\)~~</span>\n",
    "- <span style='font-size:small'>~~Basic \\(very\\) summarization of text~~</span>\n",
    "- <span style='font-size:small'>~~Basic \\(very\\) translation~~</span>\n",
    "- <span style='font-size:small'>~~Basic \\(very\\) classification~~</span>\n",
    "\n",
    "**Day 3 Objectives:**\n",
    "\n",
    "1. Finish understanding what \"Bag of Words\" is and what it can be used for\n",
    "2. Introduce TF\\-IDF and Word2Vec Embeddings\n",
    "3. Understand what is meant by vector representations of words\n",
    "4. Understand \"similarity\" measures \\(Euclidian Distance, dot product, cosine similarity\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The task at hand...\n",
    "\n",
    "Suppose for a moment, that the Math Department Head just received more than 1,500 student responses to a end of course survey. She is interested in what students perceive as HELPFUL and UNHELPFUL experiences. \n",
    "\n",
    "What kind of questions might she have?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #Valence Aware Dictionary for sEntiment Reasoning\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Course End Feedback**\n",
    "\n",
    "The $\\texttt{pd.read\\_csv}$ command below captures preprocessing tasks completed during the last meetings: \n",
    "\n",
    "- Tokenization\n",
    "- Proper Noun Deidentification\n",
    "- Lower Casing \n",
    "- Removing Digits \n",
    "- Removing Punctuation\n",
    "- Removing Whitespace\n",
    "- Spell Checking \n",
    "- Optionally remove stopwords (create 2nd corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "helpfulNew = pd.read_csv('../data/helpfulNew.csv',keep_default_na=False)\n",
    "helpfulNew.drop(columns=[\"Unnamed: 0\"], inplace = True)\n",
    "helpfulNew.head()\n",
    "helpfulNew = helpfulNew['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stopword Removal**\n",
    "\n",
    "This is the process of removing the words that are so common that they don't provide any value to understanding the text. Examples shown below.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"xxx\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "\n",
    "# create another dataframe without stopwords: helpfulNewStop\n",
    "def removeStopWords(text,stop_words):\n",
    "    text_tokens = word_tokenize(text)\n",
    "    return ' '.join([w for w in text_tokens if not w in stop_words])\n",
    "\n",
    "helpfulNewStop = helpfulNew.apply(lambda x: removeStopWords(str(x),stop_words))\n",
    "# helpfulNewStop.head()\n",
    "# print(\"Before Stop Words: \", helpfulNew[1], \"\\n\\nAfter Stop Word Removal: \",helpfulNewStop[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **So what do we mean when we say Bag of Words?**\n",
    "_Adapted from [CS287: Deep Learning for NLP](https://harvard-iacs.github.io/CS287/schedule), a course offered in Fall 2021_\n",
    "\n",
    "Extracting the occurrence of words within a document. It is a \"bag\" since we disregard the structure of a given sentence or how words interact with each other.  A model only cares about what words are used \\- not the order in which they are used.   \n",
    "\n",
    "Now, we analyze our corpus by creating *vector representations* of each document. These vector representations will allow us to work with documents as numerical data, compare them to one another, and hopefully design a useful information retrieval system.  \n",
    "\n",
    "The first vector representation we will examine is **bag-of-words** (BoW). Here, in BoW, the dimension of each vector should represents the number of times a particular word occurs in a document.\n",
    "\n",
    "**Word Mapping**\n",
    "\n",
    "To start, we need to create a **vocabulary** that maps words to the indices of a vector. The `create_vocab()` function takes as input our `corpus` of strings and returns a dictionary that maps each unique word in the corpus to a unique integer *index*. The returned dictionary should satisfy the following properties:\n",
    "\n",
    "- Each word is mapped to a number between $0$ and $V - 1$, inclusive, where $V$ is the number of unique words in the corpus.\n",
    "- Words with higher frequency across the corpus should have smaller indices; i.e. the most frequent word should have index $0$ and the least frequent word should have index $V - 1$.  Ties are broken arbitrarily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# use tokenized `corpus` and create an indexed vocabulary of every word in the corpus\n",
    "# map words to indices of a vector to create a lookup table\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Dict, List\n",
    "from pandas import Series\n",
    "\n",
    "def create_vocab(corpus: Series) -> Dict[str, int]:\n",
    "\n",
    "    # concatenate all responses to get a list of every token used in the corpus\n",
    "    all_words = corpus.str.cat(sep = ' ')\n",
    "    token_list = nltk.word_tokenize(all_words)\n",
    "\n",
    "    # use Counter to create dict of counts of all tokens\n",
    "    token_dict = Counter(token_list)\n",
    "\n",
    "    # make dict of {sorted **token_dict.most_common()** tokens : index}\n",
    "    index_dict = dict([(x[0],y) for (x,y) in zip(token_dict.most_common(),range(len(token_dict)))])\n",
    "\n",
    "    return index_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full vocab size: 796\n",
      "'the' index: 0\n",
      "'board' index: 19\n",
      "'problems' index: 12\n",
      "'are' index: 135\n",
      "'fun' index: 209\n"
     ]
    }
   ],
   "source": [
    "# sanity check cell\n",
    "full_vocab = create_vocab(helpfulNew)\n",
    "print(\"full vocab size:\", len(full_vocab))\n",
    "print(\"'the' index:\", full_vocab['the'])\n",
    "print(\"'board' index:\", full_vocab['board'])\n",
    "print(\"'problems' index:\", full_vocab['problems'])\n",
    "print(\"'are' index:\", full_vocab['are'])\n",
    "print(\"'fun' index:\", full_vocab['fun'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BoW Representations**\n",
    "\n",
    "Now, let's create the actual BoW representations. The function `create_bow()` does the following:\n",
    "- accepts two inputs: `tokenized_corpus` and the output from `create_vocab()`\n",
    "- it creates BoW representations for each comment and returns it\n",
    "- specifically, the output needs to be a dictionary mapping each document id to a NumPy array $\\vec x \\in \\mathbb{R}^V$, where $x_i$ is the number of times the word, whose index into the vocabulary is $i$, appears in the document. For example, if $i=0$ corresponds to \"the\", and \"the\" appears 17 times in the given document, then the 1st index of vector $\\vec x$ should be 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [in, class, examples, were, great, for, xxx, x...\n",
       "1      [i, particularly, thought, getting, up, to, th...\n",
       "2      [every, time, we, would, go, to, boards, we, w...\n",
       "3                      [humor, that, motivate, learning]\n",
       "4      [during, almost, every, class, period, we, got...\n",
       "                             ...                        \n",
       "220    [when, the, xxx, was, in, the, classroom, we, ...\n",
       "221    [the, note, sheets, for, grade, assignment, we...\n",
       "222    [working, through, board, problems, helped, me...\n",
       "223    [using, your, tablet, to, write, all, the, not...\n",
       "224    [very, open, to, answering, difficult, questions]\n",
       "Name: text, Length: 225, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_comments_helpful = helpfulNew.apply(lambda x: nltk.word_tokenize(str(x)))\n",
    "tokenized_comments_helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create BoW representations based on counts of words in each comment\n",
    "import numpy as np\n",
    "\n",
    "def create_bow(tokenized_corpus: Series, vocab: Dict[str, int]) -> Dict[int, np.ndarray]:\n",
    "\n",
    "    # empty dictionary to add {doc_id: bow_array} key,value pairs\n",
    "    doc_map = {}\n",
    "\n",
    "    # iterate through each comment by doc_id\n",
    "    for doc_id, tokens in tokenized_corpus.to_dict().items():\n",
    "        bow_array = np.zeros(len(vocab))#,dtype=int)\n",
    "        token_count = Counter(tokens)\n",
    "\n",
    "        # go through each word and match its count with the word's appropriate index\n",
    "        #    based on the `vocab` dictionary\n",
    "        for word,count in token_count.items():\n",
    "            #print(doc_id)\n",
    "            #if word != 'nan': # use this if your data contains nan\n",
    "            idx = vocab[word]\n",
    "            bow_array[idx] = count\n",
    "\n",
    "        # add the resultant {doc_id: bow_array} key,value pair to the returned dictionary\n",
    "        doc_map[doc_id] = bow_array\n",
    "\n",
    "    return doc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# keys: 225\n",
      "vocab size: 796\n",
      "[2. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "full_bow = create_bow(tokenized_comments_helpful, full_vocab)\n",
    "print(\"# keys:\", len(full_bow.keys()))\n",
    "print(\"vocab size:\", len(full_bow[5]))\n",
    "print(full_bow[5][:20])\n",
    "print(full_bow[10][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of comments 42 and 80: 0.19069251784911848\n",
      " * comment 42: the instructor really helped us through the project\n",
      " * comment 80: taking boards is always the best way to learn i believe\n",
      "\n",
      "\n",
      "\n",
      "cosine similarity of comments 77 and 188: 0.19399589841579265\n",
      " * 77: xxx xxx made us do a lot of board work every day to practice problems which helped me a lot she would then come by each students board and help correct them if needed\n",
      " * 188: doing real world problems associated with each lesson it really helped my understanding of how to actually apply the concepts we learned instead of just memorizing steps and functions\n"
     ]
    }
   ],
   "source": [
    "# use BoW with cosine similarity\n",
    "def cos_sim(doc1,doc2,doc_dict):\n",
    "    # get associated bow vectors of input docs\n",
    "    a = doc_dict[doc1]\n",
    "    b = doc_dict[doc2]\n",
    "\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "print(f'cosine similarity of comments 42 and 80: {cos_sim(42,80,full_bow)}')\n",
    "print(f' * comment 42: {helpfulNew.loc[42]}')\n",
    "print(f' * comment 80: {helpfulNew.loc[80]}')\n",
    "print('\\n')\n",
    "\n",
    "print()\n",
    "\n",
    "# pick two other course comments to examine\n",
    "comment_1, comment_2 = 77, 188\n",
    "print(f'cosine similarity of comments {comment_1} and {comment_2}: {cos_sim(comment_1,comment_2,full_bow)}')\n",
    "print(f' * {comment_1}: {helpfulNew.loc[comment_1]}')\n",
    "print(f' * {comment_2}: {helpfulNew.loc[comment_2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * comment 80: \t\t\ttaking boards is always the best way to learn i believe\n",
      " * contrived comment A: \ti think taking boards is the best way to learn in class\n",
      " * contrived comment B: \tassuming boards is most the greatest methods to understand problems i think\n",
      "\n",
      "cosine similarity of comments 80 and contrived example A: 0.7833494518006403\n",
      "cosine similarity of comments 80 and contrived example B: 0.4351941398892446\n"
     ]
    }
   ],
   "source": [
    "# Create a contrived string to show different cosine similarities\n",
    "# similar to comment 80: 'taking boards is always the best way to learn i believe'\n",
    "contrived_comment_a = 'i think taking boards is the best way to learn in class'\n",
    "contrived_comment_b = 'assuming boards is most the greatest methods to understand problems i think'\n",
    "contrived_tokenized_a = pd.Series(contrived_comment_a).apply(lambda x: nltk.word_tokenize(x))\n",
    "contrived_tokenized_b = pd.Series(contrived_comment_b).apply(lambda x: nltk.word_tokenize(x))\n",
    "contrived_bow_a = create_bow(contrived_tokenized_a, full_vocab)\n",
    "contrived_bow_b = create_bow(contrived_tokenized_b, full_vocab)\n",
    "\n",
    "# temporarily add contrived comment to `full_bow` dictionary\n",
    "full_bow['contrived_a'] = contrived_bow_a[0]\n",
    "full_bow['contrived_b'] = contrived_bow_b[0]\n",
    "\n",
    "print(f' * comment 80: \\t\\t\\t{helpfulNew.loc[80]}')\n",
    "print(f' * contrived comment A: \\t{contrived_comment_a}')\n",
    "print(f' * contrived comment B: \\t{contrived_comment_b}\\n')\n",
    "\n",
    "print(f'cosine similarity of comments 80 and contrived example A: {cos_sim(80,\"contrived_a\",full_bow)}')\n",
    "print(f'cosine similarity of comments 80 and contrived example B: {cos_sim(80,\"contrived_b\",full_bow)}')\n",
    "\n",
    "# # optionally remove contrived comment from `full_bow` dictionary\n",
    "# del full_bow['contrived_a']\n",
    "# del full_bow['contrived_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comment: the path project i learned a lot about modeling and real world solutions\n",
      "\n",
      "5 most similar comments based on Bag of Words cosine similarity:\n",
      " *  doing the path project\n",
      " *  the final path modeling day in this class used a lot of our skill in an advanced way but i still had a firm grasp on the material\n",
      " *  the experience of working in groups was one that i learned a lot from\n",
      " *  the xxx we worked with was helpful and very interesting and relevant to the material even though the projects we did took a long time i thought they supplemented the course well i also had not done path projects in the past it was strictly learning the material test and then the next topic\n",
      " *  i learned a lot since i was the only person who ever talked in class or asked any questions\n"
     ]
    }
   ],
   "source": [
    "def doc_sim(doc1, doc_dict, k=5):\n",
    "\n",
    "    cos_sim_dict = {}\n",
    "    for doc2 in doc_dict.keys():\n",
    "        if doc2 == doc1: \n",
    "            pass\n",
    "        else:\n",
    "            cos_sim_dict[doc2]=cos_sim(doc1,doc2,doc_dict)\n",
    "\n",
    "    # I used the following stackoverflow page to sort by dict value\n",
    "    #    https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "\n",
    "    #  {doc_id: cos_sim to given doc}; sort by value REVERSE\n",
    "    sorted_dict = {k: v for k, v in sorted(cos_sim_dict.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "    # resulting docids with closest cosine similarity\n",
    "    sorted_docids = [x for x,y in zip(sorted_dict.keys(),range(k))]\n",
    "\n",
    "\n",
    "    return sorted_docids \n",
    "\n",
    "# use function and print comments\n",
    "comment_entry = 51\n",
    "k=5\n",
    "bowsim = doc_sim(comment_entry,full_bow,k)\n",
    "print(f'Original comment: {helpfulNew.loc[comment_entry]}')\n",
    "print(f'\\n{k} most similar comments based on Bag of Words cosine similarity:')\n",
    "for d in bowsim:\n",
    "    print(f' *  {helpfulNew.loc[d]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time = 0.20882821083068848\n"
     ]
    }
   ],
   "source": [
    "def avg_doc_sim(bow_dict):\n",
    "    '''\n",
    "    Input:   BOW dictionary of doc_id: np.array of BOW vector\n",
    "    Output:  a dictionary of doc_id:average cosine similarity across comments\n",
    "    '''\n",
    "    avg_doc_sim_dict = {}\n",
    "    num_docs = len(bow_dict.keys())\n",
    "\n",
    "    for doc1 in bow_dict.keys():\n",
    "        # reset accumulator of document similarities\n",
    "        docsim = 0\n",
    "        for doc2 in bow_dict.keys():\n",
    "            if doc2 == doc1:\n",
    "                pass\n",
    "            else:\n",
    "                # accumulate total document similarity values\n",
    "                docsim+=cos_sim(doc1,doc2,bow_dict)\n",
    "\n",
    "        avg_doc_sim_dict[doc1] = docsim/(num_docs-1)\n",
    "\n",
    "    return avg_doc_sim_dict\n",
    "\n",
    "t0 = time.time()\n",
    "avg_doc_sim_full = avg_doc_sim(full_bow)\n",
    "print(f'total time = {time.time()-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=range(len(full_bow))\n",
    "x_labels = [x for x in avg_doc_sim_full.keys()]\n",
    "y = [x for x in avg_doc_sim_full.values()]\n",
    "\n",
    "plt.bar(x, y, color='green', align='center')\n",
    "plt.title('Average Document Similarity')\n",
    "plt.xticks([])\n",
    "plt.ylabel('Average Cosine Similarity')\n",
    "plt.xlabel('Individual Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment with min cos sim:  40 with 0.000\n",
      "Comment with max cos sim:  97 with 0.326\n",
      "Average cos sim across all Comments:  0.175\n",
      "\n",
      "min cos_sim comment:\n",
      "  * shoots\n",
      "\n",
      "max cos_sim comment:\n",
      "  * any time xxx xxx would go over our reading or run though the processes we studied the night before really helped to cement the concept and course material into my head i found that the most important source in the class was the teacher and the xxx in a real and xxx xxx text book\n",
      "\n",
      "Lowest 10 cos_sim values:\tHighest 10 cos_sim values:\n",
      "0.000\t\t\t\t0.277\n",
      "0.002\t\t\t\t0.282\n",
      "0.004\t\t\t\t0.282\n",
      "0.004\t\t\t\t0.287\n",
      "0.022\t\t\t\t0.289\n",
      "0.027\t\t\t\t0.291\n",
      "0.031\t\t\t\t0.292\n",
      "0.035\t\t\t\t0.298\n",
      "0.042\t\t\t\t0.300\n",
      "0.045\t\t\t\t0.326\n"
     ]
    }
   ],
   "source": [
    "sort_y = sorted(y)\n",
    "\n",
    "loc_min_y = list(avg_doc_sim_full.keys())[list(avg_doc_sim_full.values()).index(min(y))]\n",
    "print(f'Comment with min cos sim:  {loc_min_y} with {min(y):.3f}')\n",
    "\n",
    "loc_max_y = list(avg_doc_sim_full.keys())[list(avg_doc_sim_full.values()).index(max(y))]\n",
    "print(f'Comment with max cos sim:  {loc_max_y} with {max(y):.3f}')\n",
    "\n",
    "print(f'Average cos sim across all Comments:  {np.mean(y):.3f}')\n",
    "\n",
    "print('\\nmin cos_sim comment:')\n",
    "print('  * '+helpfulNew.iloc[loc_min_y])\n",
    "print('\\nmax cos_sim comment:')\n",
    "print('  * '+helpfulNew.iloc[loc_max_y])\n",
    "\n",
    "print('\\nLowest 10 cos_sim values:\\tHighest 10 cos_sim values:')\n",
    "for i,j in zip(sort_y[:10],sort_y[-10:]):\n",
    "      print(f'{i:.3f}\\t\\t\\t\\t{j:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have a comment that says \"shoots\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kahoot becomes shoot\n"
     ]
    }
   ],
   "source": [
    "# Show that TextBlob's spell checker corrected \"Kahoot\" to shoots\n",
    "test_string = 'kahoot'\n",
    "print(f'{test_string} becomes {str(TextBlob(test_string).correct())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create TF-IDF vectors**\n",
    "\n",
    "\n",
    "Instead of using BoW vectors with raw counts, let's try using term frequency-inverse document frequency (TF-IDF) vectors. Implement the `create_TFIDF()` function below. Its inputs and outputs are identical to `create_BoW()`:\n",
    "- accept two inputs: the output from `load_corpus()` (e.g., `samp_tokenized` or `full_tokenized`) and the output from `create_vocab()` (e.g., `samp_vocab` or `full_vocab`)\n",
    "- it creates TF-IDF representations for each comment and returns it. \n",
    "- specifically, the output needs to be a dictionary mapping each document id to a NumPy array $\\vec x \\in \\mathbb{R}^V$, where $x_i$ is the TF-IDF for the word whose index into the vocabulary is $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_TFIDF(tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int]) -> Dict[int, np.ndarray]:\n",
    "    \n",
    "    # use create_bow to get all word counts for each doc_id\n",
    "    bow_rep = create_bow(tokenized_texts,vocab)\n",
    "    doc_count = len(bow_rep)\n",
    "    \n",
    "    # From TF-IDF Slide\n",
    "    #    f_{w_i} = # times w_i appeared in document\n",
    "    #    TFIDF = f_{w_i} * log(# docs in corpus / # docs containing w_i)\n",
    "    \n",
    "    # make matrix `f_w` of all BoW vectors to count # of docs with w_i\n",
    "    f_w = []\n",
    "    for vect in bow_rep.values():\n",
    "        f_w.append(vect)\n",
    "    \n",
    "    f_w = np.array(f_w)\n",
    "    \n",
    "    # sum number of docs with each word by counting non-zero entries\n",
    "    #    np.count_nonzero(array, axis=0) will count non-zero entries in each column (\"across rows\")\n",
    "    word_by_doc = np.count_nonzero(f_w,axis=0)\n",
    "    idfs = np.log(doc_count/word_by_doc)\n",
    "    \n",
    "    # create dictionary with {comment_index: TF-IDF representation} key/value pairs\n",
    "    TFIDF_dict = {}\n",
    "    for i,doc in enumerate(bow_rep.keys()):\n",
    "        TFIDF_dict[doc] = f_w[i,:]*idfs\n",
    "\n",
    "    return TFIDF_dict, idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Original Comment: xxx xxx made us do a lot of board work every day to practice problems which helped me a lot she would then come by each students board and help correct them if needed\n",
      "\n",
      "** TF-IDF representation for first 12 entries of vector:\n",
      "[0.         2.01221882 0.77098967 0.77098967 0.         0.\n",
      " 2.4405148  0.         0.         1.28181529 0.         0.        ]\n",
      "\n",
      "** TF-IDF score for each unique word in comment:\n",
      "xxx\t2.0122188193696093\n",
      "made\t3.6331905482533475\n",
      "us\t2.4805110383149622\n",
      "do\t2.7168998163791924\n",
      "a\t2.440514796180873\n",
      "lot\t5.1834733468503735\n",
      "of\t1.28181529108987\n",
      "board\t3.682862158050585\n",
      "work\t1.869601955991989\n",
      "every\t3.2277254401451834\n",
      "day\t3.6331905482533475\n",
      "to\t0.7709896673238792\n",
      "practice\t2.8600006600198657\n",
      "problems\t1.533129719370776\n",
      "which\t3.0270547446830323\n",
      "helped\t1.6407603835631415\n",
      "me\t1.7113779507770948\n",
      "she\t4.326337728813293\n",
      "would\t2.785892687866144\n",
      "then\t3.3455084758015667\n",
      "come\t4.7318028369214575\n",
      "by\t3.122364924487357\n",
      "each\t3.8155121050473024\n",
      "students\t3.4790398684260895\n",
      "and\t0.7709896673238792\n",
      "help\t2.785892687866144\n",
      "correct\t4.7318028369214575\n",
      "them\t2.8600006600198657\n",
      "if\t3.3455084758015667\n",
      "needed\t3.8155121050473024\n"
     ]
    }
   ],
   "source": [
    "# first, append two contrived comments to tokenized dataframe for follow-on demo purposes \n",
    "#   because TF-IDF requires the whole corpus!\n",
    "tokenized_comments_helpful[len(tokenized_comments_helpful.index)] = contrived_tokenized_a[0]\n",
    "tokenized_comments_helpful[len(tokenized_comments_helpful.index)] = contrived_tokenized_b[0]\n",
    "\n",
    "# create TF-IDF\n",
    "helpful_tfidf, idfs = create_TFIDF(tokenized_comments_helpful, full_vocab)\n",
    "\n",
    "# look at comment 77 for insights\n",
    "print(f'\\n** Original Comment: {helpfulNew[77]}')\n",
    "# examine first 12 entries of comment 77\n",
    "print('\\n** TF-IDF representation for first 12 entries of vector:')\n",
    "print(helpful_tfidf[77][:12])\n",
    "\n",
    "# print each unique word from comment 77 with its TF-IDF score\n",
    "\n",
    "print('\\n** TF-IDF score for each unique word in comment:')\n",
    "duplicate_list = []\n",
    "for word in tokenized_comments_helpful[77]:\n",
    "    if word not in duplicate_list:\n",
    "        duplicate_list.append(word)\n",
    "        word_idx = full_vocab[word]\n",
    "        print(f'{word}\\t{helpful_tfidf[77][word_idx]}')\n",
    "    else: continue\n",
    "\n",
    "\n",
    "# for i,val in enumerate(helpful_tfidf[80][:12]):\n",
    "#     print(f'{list(full_vocab.keys())[i]}\\t{val}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine IDF scores for different words in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\t0.43451743070266635\n",
      "xxx\t1.0061094096848047\n",
      "to\t0.7709896673238792\n",
      "and\t0.7709896673238792\n",
      "in\t0.8197798314933114\n",
      "class\t1.0682411907918108\n",
      "a\t1.2202573980904365\n",
      "i\t1.2660669341217308\n",
      "was\t1.235295275454977\n",
      "of\t1.28181529108987\n",
      "we\t1.4359659709171282\n",
      "helpful\t1.533129719370776\n",
      "\n",
      "back\t4.038655656361512\n",
      "final\t4.038655656361512\n",
      "understood\t4.038655656361512\n",
      "solving\t4.038655656361512\n",
      "end\t4.326337728813293\n",
      "apply\t4.038655656361512\n",
      "teach\t4.326337728813293\n",
      "encouraged\t4.326337728813293\n",
      "instead\t4.326337728813293\n",
      "test\t4.326337728813293\n",
      "knowledge\t4.326337728813293\n",
      "showed\t4.326337728813293\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(f'{list(full_vocab.keys())[i]}\\t{idfs[i]}')\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(188,200):\n",
    "    print(f'{list(full_vocab.keys())[i]}\\t{idfs[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original comment: the path project i learned a lot about modeling and real world solutions\n",
      "\n",
      "5 most similar comments based on TF-IDF cosine similarity:\n",
      " *  xxx work on path modeling days\n",
      " *  doing the path project\n",
      " *  trying to show real world application of path\n",
      " *  xxx project\n",
      " *  the experience of working in groups was one that i learned a lot from\n"
     ]
    }
   ],
   "source": [
    "# see 5 nearest neighbors to a particular comment\n",
    "comment_entry = 51\n",
    "k=5\n",
    "tfidf_sim = doc_sim(comment_entry,helpful_tfidf,k)\n",
    "print(f'Original comment: {helpfulNew.loc[comment_entry]}')\n",
    "print(f'\\n{k} most similar comments based on TF-IDF cosine similarity:')\n",
    "for d in tfidf_sim:\n",
    "    print(f' *  {helpfulNew.loc[d]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * comment 80: \t\t\ttaking boards is always the best way to learn i believe\n",
      " * contrived comment A: \ti think taking boards is the best way to learn in class\n",
      " * contrived comment B: \tassuming boards is most the greatest methods to understand problems i think\n",
      "\n",
      "cosine similarity of comments 84 and contrived example A: 0.7771950808000737\n",
      "cosine similarity of comments 84 and contrived example B: 0.1458279504712885\n"
     ]
    }
   ],
   "source": [
    "# Create a contrived string to show different cosine similarities using TF-IDF\n",
    "# similar to comment 84: 'taking boards is always the best way to learn i believe'\n",
    "# contrived_comment_a = 'i think taking boards is the best way to learn in class'\n",
    "# contrived_comment_b = 'assuming boards is most the greatest methods to understand problems i think'\n",
    "\n",
    "print(f' * comment 80: \\t\\t\\t{helpfulNew.loc[80]}')\n",
    "print(f' * contrived comment A: \\t{contrived_comment_a}')\n",
    "print(f' * contrived comment B: \\t{contrived_comment_b}\\n')\n",
    "\n",
    "# contrived_comment_a is at location 225; contrived_comment_b is at location 226\n",
    "print(f'cosine similarity of comments 84 and contrived example A: {cos_sim(80,225,helpful_tfidf)}')\n",
    "print(f'cosine similarity of comments 84 and contrived example B: {cos_sim(80,226,helpful_tfidf)}')\n",
    "\n",
    "# optionally remove contrived comment from `full_bow` dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word2Vec Embeddings**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google News has its own vectors and vocabulary. Below, we use these objects, along with your `tokenized_comments_helpful` to implement the following function `run_comment2vec()`, which assigns a vector to every comment based on the Word2Vec word vectors. If a word in the corpus happens to be missing from the Google News vocabulary, we skip it when computing document vectors [note: the denominator in the average is adjusted accordingly].\n",
    "\n",
    "The function accepts as inputs:\n",
    "- the output from `load_corpus()` (e.g., `samp_tokenized`)\n",
    "- `gnews_vocab` (assigned in first cell at top of notebook)\n",
    "- `gnews_vecs` (assigned in first cell at top of notebook)\n",
    "\n",
    "It returns a dictionary, just like `create_TFIDF()`, whereby the keys are the doc ids and the values are the corresponding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import load time: 26.33954381942749\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "t0 = time.time()\n",
    "gnews = api.load('word2vec-google-news-300')\n",
    "gnews_vecs = gnews.vectors\n",
    "gnews_vocab = gnews.key_to_index\n",
    "total_time = time.time() - t0\n",
    "print(f'Import load time: {total_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar tokens (using pretrained model) to CHALKBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blackboard', 0.8274536728858948),\n",
       " ('chalkboards', 0.6927468180656433),\n",
       " ('whiteboard', 0.6784566044807434),\n",
       " ('dry_erase', 0.6494746208190918),\n",
       " ('fingernails_scratching', 0.6353917121887207),\n",
       " ('macadamia_pursuant', 0.6210680603981018),\n",
       " ('blackboards', 0.5984241366386414),\n",
       " ('fingernails_scraping', 0.5554476380348206),\n",
       " ('Chalkboards', 0.5303213000297546),\n",
       " ('greaseboard', 0.5270883440971375)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnews.most_similar(\"chalkboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comment2vec(tokenized_texts: Dict[int, List[str]], vocab: Dict[str, int], vecs: np.ndarray) -> Dict[int, np.ndarray]:\n",
    "    doc_vecs = {}\n",
    "\n",
    "    # using the word2vec word embeddings, take the mean of the word embeddings to form an\n",
    "    # average embedding to represent the whole pdr comment\n",
    "    for doc_id, doc in tokenized_texts.items():\n",
    "        doc_vecs[doc_id] = np.mean([vecs[vocab[word]] for word in doc if word in vocab], axis=0)\n",
    "    return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "comment2vec = run_comment2vec(tokenized_comments_helpful, gnews_vocab, gnews_vecs)\n",
    "\n",
    "# check the shape -- this is the mean of all of the word embeddings for a single PDR comment\n",
    "# 300 is the vector length for these particular word2vec embeddings\n",
    "print(comment2vec[80].shape)\n",
    "# print(comment2vec[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment2vec_df = pd.DataFrame(comment2vec).T\n",
    "comment2vec_norms = np.sqrt((comment2vec_df ** 2).sum(axis=1))\n",
    "comment2vec_cosines = comment2vec_df @ comment2vec_df.T / comment2vec_norms.values / comment2vec_norms.values.reshape(-1, 1)\n",
    "comment2vec_cosines\n",
    "\n",
    "# Get top 5 recommendations for comment2vec\n",
    "comment2vec_recs = comment2vec_cosines.apply(lambda x: pd.Series(x.nlargest(6).index[1:])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Query ****\n",
      "60 \t getting the chance to work with our classmates on the boards and then ask xxx xxx if we had any questions was helpful\n",
      "\n",
      "**** comment2vec Recommendations ****\n",
      "179 \t xxx xxx would always walk around and check our boards while we did board work if we weren understanding something he would carefully explain it to us until we did\n",
      "77 \t xxx xxx made us do a lot of board work every day to practice problems which helped me a lot she would then come by each students board and help correct them if needed\n",
      "97 \t any time xxx xxx would go over our reading or run though the processes we studied the night before really helped to cement the concept and course material into my head i found that the most important source in the class was the teacher and the xxx in a real and xxx xxx text book\n",
      "1 \t i particularly thought getting up to the boards as soon as xxx xxx had a good feeling about our understanding of a concept was helpful i think xxx xxx did a pretty good job at balancing getting us to think to understand concepts and teach it to help us understand\n",
      "174 \t doing practice problems that are similar to the ones on the xxx normally in class the questions are significantly easier than the xxx so when we do an advance question it helps more on the xxx\n",
      "\n",
      "**** TF-IDF Recommendations ****\n",
      "100 \t xxx xxx held xxx in which we could ask any questions we needed answers to\n",
      "1 \t i particularly thought getting up to the boards as soon as xxx xxx had a good feeling about our understanding of a concept was helpful i think xxx xxx did a pretty good job at balancing getting us to think to understand concepts and teach it to help us understand\n",
      "49 \t doing individual board work this allowed us the chance to do the work on our feet literally and figuratively and get our work corrected by our instructor\n",
      "204 \t in the beginning of the smelter we had a lot of group work where we could interact and ask questions from my instructor\n",
      "89 \t helpful experiences include the worksheets given in class with the practice problems going to boards and discussing with classmates was also helpful\n",
      "\n",
      "**** BoW Recommendations ****\n",
      "97 \t any time xxx xxx would go over our reading or run though the processes we studied the night before really helped to cement the concept and course material into my head i found that the most important source in the class was the teacher and the xxx in a real and xxx xxx text book\n",
      "169 \t having in class xxx was helpful the day before xxx when we have access to the teacher during the problem solving lab and the day before the course project was due having time to peer review and ask questions in class was valuable\n",
      "75 \t xxx work at the boards\n",
      "144 \t the xxx we worked with was helpful and very interesting and relevant to the material even though the projects we did took a long time i thought they supplemented the course well i also had not done path projects in the past it was strictly learning the material test and then the next topic\n",
      "174 \t doing practice problems that are similar to the ones on the xxx normally in class the questions are significantly easier than the xxx so when we do an advance question it helps more on the xxx\n",
      "\n",
      "\n",
      "**** Query ****\n",
      "9 \t doing practice problems and having class examples were very helpful\n",
      "\n",
      "**** comment2vec Recommendations ****\n",
      "26 \t any type of board work or practice problems in class was very beneficial\n",
      "89 \t helpful experiences include the worksheets given in class with the practice problems going to boards and discussing with classmates was also helpful\n",
      "199 \t i got a lot of helpful practice from doing board problems with a partner\n",
      "5 \t demonstrations and example problems done by the instructor on the boards were very helpful\n",
      "13 \t one helpful experience was doing board problems\n",
      "\n",
      "**** TF-IDF Recommendations ****\n",
      "211 \t doing board problems\n",
      "214 \t having xxx help with peers in class\n",
      "123 \t in the classroom going over lessons on the board and doing practice problems helped greatly to understand the concepts behind the tasks we were doing\n",
      "199 \t i got a lot of helpful practice from doing board problems with a partner\n",
      "13 \t one helpful experience was doing board problems\n",
      "\n",
      "**** BoW Recommendations ****\n",
      "5 \t demonstrations and example problems done by the instructor on the boards were very helpful\n",
      "211 \t doing board problems\n",
      "13 \t one helpful experience was doing board problems\n",
      "89 \t helpful experiences include the worksheets given in class with the practice problems going to boards and discussing with classmates was also helpful\n",
      "26 \t any type of board work or practice problems in class was very beneficial\n",
      "\n",
      "\n",
      "**** Query ****\n",
      "14 \t the project forced us to work together and think clinically it gave me a better idea of who i could go to for help\n",
      "\n",
      "**** comment2vec Recommendations ****\n",
      "52 \t i enjoyed when we would get called up to solve a problem and if we did not know how to solve it other students would help us out this was beneficial because it allowed us to get rep and stay engaged in class especially on days when it was hard to stay engaged\n",
      "178 \t i really enjoyed the board activities that we did i think that was the experience within class that benefited my learning it offered a chance for us to practice the material collaborate with others and have the teacher assist if needed\n",
      "113 \t one time we broke apart into two groups with learning marines so that people could get extra help or people could work ahead which i found was a good use of my time in class to not have to go over things i already had a good handle on\n",
      "54 \t i like the board work that we did in every class it helped me get a better understanding of the material that we were covering for that days lesson\n",
      "142 \t xxx xxx was very good about challenging us while still giving us help as needed at least after the first problem set he was very understanding towards our level of knowledge he had us code a lot but was always willing to look at our code or work on it in class also really love the life stories\n",
      "\n",
      "**** TF-IDF Recommendations ****\n",
      "217 \t i think that when we did break down into groups and work together it was helpful\n",
      "121 \t i really liked the coming on xxx it definitely made me think more clinically and i was always really excited to tackle the problem\n",
      "215 \t xxx project\n",
      "127 \t i believe that working in tears was a very important part in this path class it allowed for the students to build a better understanding of the material and work with peers who may have a different perspective on problem solving\n",
      "12 \t doing board work was always helpful since then my classmates and teacher could see where i was missing up and could help\n",
      "\n",
      "**** BoW Recommendations ****\n",
      "1 \t i particularly thought getting up to the boards as soon as xxx xxx had a good feeling about our understanding of a concept was helpful i think xxx xxx did a pretty good job at balancing getting us to think to understand concepts and teach it to help us understand\n",
      "86 \t a helpful class experience i had was going to the board after learning the material to try and work out a problem with my small group of peers this allowed us to determine how well we grasped the topic and if we had to spend more time practicing this for future exam\n",
      "65 \t the instructor was able to manage the scope of the course and its objectives he was able to relate it to his life and enter us all personally to ensure we are receiving the material\n",
      "127 \t i believe that working in tears was a very important part in this path class it allowed for the students to build a better understanding of the material and work with peers who may have a different perspective on problem solving\n",
      "152 \t when we had a quit where we told simple instructions in the execution of it and xxx xxx walked out and we all had to deliberately work together to work the quit that engaged my attention and comprehension of the work i retained that information well\n",
      "\n",
      "\n",
      "**** Query ****\n",
      "16 \t working board problems is an extremely helpful part of this course i learn a lot from both my professor and my fellow students\n",
      "\n",
      "**** comment2vec Recommendations ****\n",
      "98 \t everytime working on the boards with my group members was very beneficial to me and my learning\n",
      "58 \t i was skeptically about working board problems in the beginning but i quickly realized how much more helpful it was than simply working the problem in my notes as board work promoted working with my peers which not only helped me understand the problem better but also created an environment in which i was much more comfortable asking my peers for assistance when needed and vice versa\n",
      "178 \t i really enjoyed the board activities that we did i think that was the experience within class that benefited my learning it offered a chance for us to practice the material collaborate with others and have the teacher assist if needed\n",
      "198 \t our teacher always had relevant realworld problems so we had a sense of how we could actually apply the path we learn in real life xxx xxx is a great teacher and his efforts to prepare for his class is obvious i hope more teachers had his work ethics and desire to actually teach students\n",
      "86 \t a helpful class experience i had was going to the board after learning the material to try and work out a problem with my small group of peers this allowed us to determine how well we grasped the topic and if we had to spend more time practicing this for future exam\n",
      "\n",
      "**** TF-IDF Recommendations ****\n",
      "199 \t i got a lot of helpful practice from doing board problems with a partner\n",
      "143 \t the experience of working in groups was one that i learned a lot from\n",
      "62 \t completing board work in groups allowed me to learn from both my peers and instructor when trying to grasp the idea of marines and social network\n",
      "196 \t xxxorking with my second god group xxx xxx and xxx xxx were extremely helpful i learned a lot from the two of them\n",
      "127 \t i believe that working in tears was a very important part in this path class it allowed for the students to build a better understanding of the material and work with peers who may have a different perspective on problem solving\n",
      "\n",
      "**** BoW Recommendations ****\n",
      "199 \t i got a lot of helpful practice from doing board problems with a partner\n",
      "196 \t xxxorking with my second god group xxx xxx and xxx xxx were extremely helpful i learned a lot from the two of them\n",
      "58 \t i was skeptically about working board problems in the beginning but i quickly realized how much more helpful it was than simply working the problem in my notes as board work promoted working with my peers which not only helped me understand the problem better but also created an environment in which i was much more comfortable asking my peers for assistance when needed and vice versa\n",
      "62 \t completing board work in groups allowed me to learn from both my peers and instructor when trying to grasp the idea of marines and social network\n",
      "143 \t the experience of working in groups was one that i learned a lot from\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display closest comments given the average word2vec embedding\n",
    "for comment_id in [60, 9, 14, 16]:\n",
    "    print('**** Query ****')\n",
    "    print(comment_id,'\\t',helpfulNew.iloc[comment_id])\n",
    "    \n",
    "    print('\\n**** comment2vec Recommendations ****')\n",
    "    for rec_id in comment2vec_recs.loc[comment_id]:\n",
    "        print(rec_id,'\\t',helpfulNew.loc[rec_id])\n",
    "\n",
    "    print('\\n**** TF-IDF Recommendations ****')\n",
    "    for rec_id in doc_sim(comment_id,helpful_tfidf,k):\n",
    "        print(rec_id,'\\t',helpfulNew.loc[rec_id])\n",
    "\n",
    "    print('\\n**** BoW Recommendations ****')\n",
    "    for rec_id in doc_sim(comment_id,full_bow,k):\n",
    "        print(rec_id,'\\t',helpfulNew.loc[rec_id])\n",
    "\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** COMMENTS ***\n",
      " * comment 80: \t\t\ttaking boards is always the best way to learn i believe\n",
      " * contrived comment A: \ti think taking boards is the best way to learn in class\n",
      " * contrived comment B: \tassuming boards is most the greatest methods to understand problems i think\n",
      "\n",
      "*** Bag of Words ***\n",
      "cosine similarity of comments 80 and contrived example A: 0.7833494518006403\n",
      "cosine similarity of comments 80 and contrived example B: 0.4351941398892446\n",
      "\n",
      "*** TF-IDF ***\n",
      "cosine similarity of comments 80 and contrived example A: 0.7771950808000737\n",
      "cosine similarity of comments 80 and contrived example B: 0.1458279504712885\n",
      "\n",
      "*** Word2Vec ***\n",
      "cosine similarity of comments 80 and contrived example A: 0.9489344954490662\n",
      "cosine similarity of comments 80 and contrived example B: 0.8368156552314758\n"
     ]
    }
   ],
   "source": [
    "print('*** COMMENTS ***')\n",
    "print(f' * comment 80: \\t\\t\\t{helpfulNew.loc[80]}')\n",
    "print(f' * contrived comment A: \\t{contrived_comment_a}')\n",
    "print(f' * contrived comment B: \\t{contrived_comment_b}\\n')\n",
    "\n",
    "print('*** Bag of Words ***')\n",
    "print(f'cosine similarity of comments 80 and contrived example A: {cos_sim(80,\"contrived_a\",full_bow)}')\n",
    "print(f'cosine similarity of comments 80 and contrived example B: {cos_sim(80,\"contrived_b\",full_bow)}\\n')\n",
    "\n",
    "print('*** TF-IDF ***')\n",
    "print(f'cosine similarity of comments 80 and contrived example A: {cos_sim(80,225,helpful_tfidf)}')\n",
    "print(f'cosine similarity of comments 80 and contrived example B: {cos_sim(80,226,helpful_tfidf)}\\n')\n",
    "\n",
    "print('*** Word2Vec ***')\n",
    "print(f'cosine similarity of comments 80 and contrived example A: {comment2vec_cosines[80][225]}')\n",
    "print(f'cosine similarity of comments 80 and contrived example B: {comment2vec_cosines[80][226]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preview of transformer-based transfer learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these lines if you are operating on Google Colab or other cloud-based platform without a virtual environment.\n",
    "# !pip install datasets\n",
    "# !pip install transformers\n",
    "# !pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john.scudder\\Anaconda3\\envs\\aors\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoConfig, AutoModel, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using: \" \"GPU\" if device== 0 else \"CPU\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "classifier = pipeline(\"sentiment-analysis\",model = MODEL, device = device) # device should make use of GPUs if set to 0... if -1 will default to CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment:  in class examples were great for xxx xxx and xxx xxx to use for quires to see what material was essential to know for xxx \n",
      " [{'label': 'positive', 'score': 0.9424242973327637}]\n",
      "Comment:  i particularly thought getting up to the boards as soon as xxx xxx had a good feeling about our understanding of a concept was helpful i think xxx xxx did a pretty good job at balancing getting us to think to understand concepts and teach it to help us understand \n",
      " [{'label': 'positive', 'score': 0.9298580884933472}]\n",
      "Comment:  This was the worst class I've ever had. The work was tedious and not related to my major. \n",
      " [{'label': 'negative', 'score': 0.9576689004898071}]\n"
     ]
    }
   ],
   "source": [
    "print(\"Comment: \", helpfulNew[0],\"\\n\",classifier(helpfulNew[0]))\n",
    "print(\"Comment: \", helpfulNew[1],\"\\n\",classifier(helpfulNew[1]))\n",
    "negCom = \"This was the worst class I've ever had. The work was tedious and not related to my major.\"\n",
    "print(\"Comment: \", negCom,\"\\n\",classifier(negCom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a lesson about the incredible power of Transformers',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9968070983886719, 0.0021586946677416563, 0.0010341808665543795]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",model=\"facebook/bart-large-mnli\")\n",
    "classifier(\"This is a lesson about the incredible power of Transformers\",candidate_labels=[\"education\", \"politics\", \"business\"],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Once we leave this class about the benefits of healthy living, \\xa0my favorite line of thinking is that no one would be surprised if we had a more stable existence after living in a more loving environment.\\nBut what about the more common conclusion of'},\n",
       " {'generated_text': \"Once we leave this class about the benefits of healthy living, \\xa0that's when it really starts to make sense to consider what we can do to help those who have the greatest potential who don't.\\nPosted by The author at 07:59\"},\n",
       " {'generated_text': \"Once we leave this class about the benefits of healthy living, \\xa0we'll see how we can really make an interesting difference in that area!\\nAnd if you would like to be on the receiving end of our ideas for making your own meals,\"},\n",
       " {'generated_text': \"Once we leave this class about the benefits of healthy living, \\xa0we have now started to look further at our understanding of the benefits of eating well. Here's what happens when you're willing to let yourself be consumed by the best foods you can\"},\n",
       " {'generated_text': 'Once we leave this class about the benefits of healthy living, \\xa0I can see why you may find that this is not the end of your book. \\xa0I have no objections about your use of \"natural\" and \"natural-created\"'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model = 'gpt2')\n",
    "generator(\"Once we leave this class about the benefits of healthy living, \", max_length=50, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.5722203850746155,\n",
       "  'token': 6364,\n",
       "  'token_str': 'processing',\n",
       "  'sequence': 'natural language processing is a really cool analytical tool.'},\n",
       " {'score': 0.08222448080778122,\n",
       "  'token': 4106,\n",
       "  'token_str': 'analysis',\n",
       "  'sequence': 'natural language analysis is a really cool analytical tool.'},\n",
       " {'score': 0.038689740002155304,\n",
       "  'token': 7654,\n",
       "  'token_str': 'acquisition',\n",
       "  'sequence': 'natural language acquisition is a really cool analytical tool.'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model='bert-base-uncased')\n",
    "unmasker(\"Natural Language [MASK] is a really cool analytical tool.\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\john.scudder\\Anaconda3\\envs\\aors\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99890816,\n",
       "  'word': 'Jack',\n",
       "  'start': 11,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9965072,\n",
       "  'word': 'US Military Academy',\n",
       "  'start': 34,\n",
       "  'end': 53},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.63952595,\n",
       "  'word': 'Math Department',\n",
       "  'start': 61,\n",
       "  'end': 76}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model = \"dslim/bert-base-NER\",grouped_entities=True)\n",
    "ner(\"My name is Jack and I work at the US Military Academy in the Math Department.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.47534841299057007,\n",
       " 'start': 34,\n",
       " 'end': 76,\n",
       " 'answer': 'US Military Academy in the Math Department'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\", model = \"distilbert-base-cased-distilled-squad\")\n",
    "question_answerer(\n",
    "    question=\"Where am I employed?\",\n",
    "    context=\"My name is Jack and I work at the US Military Academy in the Math Department.\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The American Journal of Clinical Nutrition features the work of the authors of a new paper . They use state-of-the-art techniques to accurately categorize food labels . The authors say they can do this without the help of a human reading and interpreting the data . The study is published in the journal American Journal Of Clinical Nutrition, published in March 2013 .'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model = \"sshleifer/distilbart-cnn-12-6\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "Many years ago, as a student in a research methods course, I heard a most disappointing truth 8 that much of \n",
    "research is getting your data in order. Sigh. Certainly, the design, data collection, and 9 analysis phases \n",
    "are all key (and exciting) parts, but most research will include a requirement to convert 10 raw data into a \n",
    "more usable format. While many have accepted that this important, time consuming, 11 and unsatisfying part of \n",
    "research is just part of the process, this issue of The American Journal of Clinical 12 Nutrition features \n",
    "the work of the work of Hu, Ahmed, and L’Abbe [1] that demonstrates a viable way 13 to reduce this data \n",
    "wrangling burden. In their paper, the authors have applied state-of-the-art Natural 14 Language Processing \n",
    "(NLP) and Machine Learning (ML) techniques to accurately categorize food label 15 information. Demonstrating \n",
    "the impact of using these techniques, they use the University of Toronto 16 Food Label Information and Price \n",
    "(FLIP) database to identify a given label’s category (i.e. baked goods, 17 beverage, etc.) along with its \n",
    "nutritional quality. What is incredible about these results is the level of 18 accuracy that the authors \n",
    "achieve without the aid of a human reading and interpreting each label.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\john.scudder\\Anaconda3\\envs\\aors\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Greetings fellow natural language processors! I hope you are having a great class.'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install transformers[sentencepiece]\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
    "translator(\"Saludos compañeros procesadores de lenguaje natural! Espero que usted está teniendo una gran clase.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
